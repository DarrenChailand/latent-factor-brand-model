# Brand Perception Mapping Pipeline (LLM → Brand×Attribute → PMI → SVD)

This repo builds a **Brand × Attribute** matrix from LLM-generated text, then derives a latent factor representation using **PMI + SVD**, and finally computes **brand–attribute importance scores**.

The Jupyter notebook (`brand_attribute_pipeline.ipynb`) **starts from already-generated response data**.  
Before running the notebook, you must first generate:

1) prompt files (Appendix A & B)  
2) response files (Appendix A & B) using a local LLM (Ollama)

---

## Repository structure (expected)

- `data/raw/`
  - `demo_brand_prompt_config.json` (brands, industries, factors, personas, etc.)
  - `brand_prompt_templates.json` (prompt templates for Appendix A & B)
- `data/processed/prompts/`
  - `Appendix_A_generated_prompts.json`
  - `Appendix_B_generated_prompts.json`
- `data/processed/responses/`
  - `Appendix_A_responses.jsonl`
  - `Appendix_B_responses.jsonl`
- `data/processed/brand_attribute_matrix/`
  - pipeline outputs (raw matrix, filtered, normalized, pmi, svd, importance, etc.)
- `src/data_analysis/`
  - analysis scripts (build matrix, filter, normalize, PMI, SVD, importance)
- `brand_attribute_pipeline.ipynb`
  - runs the analysis once responses exist

---

## Setup

### 1) Create environment and install dependencies

```bash
python -m venv .venv
source .venv/bin/activate   # macOS/Linux
# .venv\Scripts\activate    # Windows

pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

### 2) Install and run Ollama (required for local generation)

You need Ollama installed and running.

Models used in this repo (based on your scripts):

- `llama3.2:1b` (response generation)
- `gemma3:4b` (attribute filtering/grouping)

Make sure these models are available locally:

```bash
ollama pull llama3.2:1b
ollama pull gemma3:4b
```

---

## Step-by-step pipeline

### Step A — Define brands + prompt configuration

Edit:

- `data/raw/demo_brand_prompt_config.json`  
  brands, industries, comparison_factors, personas, etc.

- `data/raw/brand_prompt_templates.json`  
  contains `brand_perception_templates` (Appendix A)  
  contains `latent_need_templates` (Appendix B)

---

### Step B — Generate prompts (Appendix A & B)

Run your two prompt-generation scripts (or equivalent code blocks) to produce:

- `data/processed/prompts/Appendix_A_generated_prompts.json`
- `data/processed/prompts/Appendix_B_generated_prompts.json`

These prompts are generated by filling template placeholders using:

- brands / industries / factors / personas / temporal modifiers (Appendix A)
- product categories / usage contexts / demographic targets / temporal framing (Appendix B)

---

### Step C — Generate responses using Ollama (Appendix A & B)

Run your response-generation script to produce:

- `data/processed/responses/Appendix_A_responses.jsonl`
- `data/processed/responses/Appendix_B_responses.jsonl`

Each line in JSONL includes:

- index, prompt, response, timestamps, etc.

This script supports resume by counting existing JSONL lines.

---

### Step D — Run the analysis notebook

Now that responses exist, open:

- `brand_attribute_pipeline.ipynb`

The notebook runs the analysis pipeline:

- Build Brand × Attribute matrix from `Appendix_A_responses.jsonl`
- Filter attributes (LLM-based keep/drop)
- Normalize/group attributes (LLM-based clustering)
- Compute PMI matrix
- Run SVD on PMI
- Compute brand–attribute importance + per-brand ranked attributes

Outputs are written under:

- `data/processed/brand_attribute_matrix/`

---

## Notes on filtering backend (OpenAI vs Ollama)

The attribute filtering step supports:

- OpenAI (ChatGPT) **OR**
- Ollama local model

This is controlled inside `filter_attributes_with_llm.py` via:

- `USE_OPENAI = True/False`

If using OpenAI, you will be prompted for an API key during execution.

---

## Reproducibility

- All generated artifacts are stored in `data/processed/`
- The pipeline is deterministic except for:
  - the LLM response generation
  - the LLM-based filtering/grouping (model-dependent)

---

## Troubleshooting

### spaCy model missing

```bash
python -m spacy download en_core_web_sm
```

### Ollama connection errors

- Confirm Ollama is installed and running
- Run `ollama list` to verify models are available

### Empty / invalid JSON from LLM

Your normalization script already includes robust JSON extraction (`safe_parse_json`).

If it still fails, try:
- reducing chunk size
- switching models
